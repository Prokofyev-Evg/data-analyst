{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3441c822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import urllib.parse\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd77e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_judges(title, date, place, online_table_link):\n",
    "    if online_table_link is None:\n",
    "        return None\n",
    "    df = pd.DataFrame(columns=['title','date','place','online_link','category', 'segment', 'position', 'name'])\n",
    "    page = requests.get(online_table_link)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    all_off = soup.find_all('a', {'href': re.compile(r'SEG[0-9]{3}OF')})\n",
    "    qty = 0\n",
    "    for link in all_off:\n",
    "        off_link = urllib.parse.urljoin(online_table_link, link.get('href'))\n",
    "        page = requests.get(off_link)\n",
    "        if page.status_code == 200:\n",
    "            soup = BeautifulSoup(page.text, 'html.parser')\n",
    "            if len(soup.find_all('h2')) < 2:\n",
    "                continue\n",
    "            category, segment = [x.strip() for x in soup.find_all('h2')[1].text.split('-')[:2]]\n",
    "            lines = soup.find_all('tr')\n",
    "            for line in lines[1:]:\n",
    "                cells = line.find_all('td')\n",
    "                if cells[0].text != '\\xa0':\n",
    "                    df.loc[len(df)] = [title, date, place, off_link, category, segment, cells[0].text, cells[1].text]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad7bcb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_links(online_table_link):\n",
    "    if online_table_link is None:\n",
    "        return None\n",
    "    page = requests.get(online_table_link)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    all_res = soup.find_all('a', {'href': re.compile(r'SEG[0-9]{3}.HTM')})\n",
    "    return list(map(lambda x: urllib.parse.urljoin(online_table_link, x.get('href')), all_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2882b5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_officials(online_table_link, data, info):\n",
    "    if online_table_link is None:\n",
    "        return None\n",
    "    page = requests.get(online_table_link)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    link = soup.find('a', {'href': re.compile(r'SEG[0-9]{3}OF.HTM')}).get('href')\n",
    "    if link is None:\n",
    "        return None\n",
    "    link = urllib.parse.urljoin(\n",
    "        online_table_link, \n",
    "        link\n",
    "    )\n",
    "    page = requests.get(link)\n",
    "    if page.status_code != 200:\n",
    "        return None\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    lines = soup.find_all('tr')\n",
    "    for line in lines:\n",
    "        cells = line.find_all('td')\n",
    "        if len(cells) > 1:\n",
    "            if cells[0].text != \"\":\n",
    "                data['function'].append(cells[0].text)\n",
    "                data['name'].append(cells[1].text)\n",
    "                for key in info:\n",
    "                    data[key].append(info[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f70b3dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_result_line(line, data):\n",
    "    cells = line.find_all('td')\n",
    "    if len(cells) < 10:\n",
    "        return False\n",
    "    rank = cells[0].text\n",
    "    full_name = cells[1].a.text\n",
    "    club = cells[1].text[len(full_name):]\n",
    "    full_name = full_name.replace(\"ё\", \"е\").replace(\"Ё\", \"Е\")\n",
    "    club = club.replace(\"ё\", \"е\").replace(\"Ё\", \"Е\")\n",
    "    full_name = [x.strip() for x in full_name.split()]\n",
    "    if len(full_name) == 2:\n",
    "        first_name = full_name[0]\n",
    "        middle_name = \"\"\n",
    "        last_name = full_name[1]\n",
    "    elif len(full_name) == 3:\n",
    "        first_name = full_name[0]\n",
    "        middle_name = full_name[1]\n",
    "        last_name = full_name[2]\n",
    "    else:\n",
    "        return False\n",
    "    tss = float(cells[2].text)\n",
    "    tes = float(cells[3].text)\n",
    "    pcs = float(cells[5].text)\n",
    "    data['rank'].append(rank)\n",
    "    data['firstname'].append(first_name)\n",
    "    data['middlename'].append(middle_name)\n",
    "    data['lastname'].append(last_name)\n",
    "    data['club'].append(club)\n",
    "    data['tss'].append(tss)\n",
    "    data['tes'].append(tes)\n",
    "    data['pcs'].append(pcs)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43967893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_and_segment(page):\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    if len(soup.find_all('h2')) >= 1:\n",
    "        return  [x.strip() for x in soup.find_all('h2')[1].text.split(' - ')[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e51db227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_results(entries_link, data, info):\n",
    "    if entries_link is None:\n",
    "        return None\n",
    "    page = requests.get(entries_link)\n",
    "    if page.status_code != 200:\n",
    "        return None\n",
    "    category, segment = get_category_and_segment(page)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    lines = soup.find_all('tr')\n",
    "    for line in lines:\n",
    "        if parse_result_line(line, data):\n",
    "            data['category'].append(category)\n",
    "            data['segment'].append(segment)\n",
    "            for key in info:\n",
    "                data[key].append(info[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b6e3bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_event(event_link, data, judges):\n",
    "    soup = BeautifulSoup(requests.get(event_link).text, 'html.parser')\n",
    "    title = soup.find(\"h1\", \"entry-title\").text\n",
    "    date = soup.find(\"div\", \"competition-date\").p.text.split('-')[0].strip()\n",
    "    place = soup.find(\"div\", \"competition-place\").p.span.text.strip()\n",
    "    online = soup.find(\"div\", \"competition-file\")\n",
    "    if online is not None:\n",
    "        online_link = online.a.get('href')\n",
    "        online_link = online_link if online_link[-1] == '/' else online_link + '/'\n",
    "        parse_officials(online_link, judges, {'date': date, 'place': place, 'online': online_link})\n",
    "        segments = get_all_links(online_link)\n",
    "        for segment in segments:\n",
    "            parse_results(segment, data, {'date': date, 'place': place, 'online': online_link})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36f9816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_season(season_link, data, judges):\n",
    "    page = requests.get(season_link)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    competitions = soup.find_all('tr')\n",
    "    for i in tqdm(range(len(competitions))): \n",
    "        title = competitions[i].find_all('td')[1]\n",
    "        if title.a is not None:\n",
    "            online_link = None\n",
    "            proto_link = None\n",
    "            event_link = title.a.get('href')\n",
    "            parse_event(event_link, data, judges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e7d1f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "season - 24-25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad4ab832cb34af7aee76dd3aab0558e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for season in range(2024, 2025):\n",
    "    data = {\n",
    "        'date': [],\n",
    "        'place': [], \n",
    "        'online':[],\n",
    "        'category': [],\n",
    "        'segment': [],\n",
    "        'rank': [],\n",
    "        'firstname': [],\n",
    "        'middlename': [],\n",
    "        'lastname': [],\n",
    "        'club': [],\n",
    "        'tss': [],\n",
    "        'tes': [],\n",
    "        'pcs': []\n",
    "    }\n",
    "    judges = {\n",
    "        'date': [],\n",
    "        'place': [], \n",
    "        'online':[],\n",
    "        'function': [],\n",
    "        'name': [],\n",
    "    }\n",
    "    print(f\"season - {str(season)[-2:]}-{str(season+1)[-2:]}\")\n",
    "    parse_season(f\"http://ffkkmo.ru/calendar/?season={season}\", data, judges)\n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    df.to_csv(f\"ffkkmo_{str(season)[-2:]}{str(season+1)[-2:]}_dump.csv\")\n",
    "    df = pd.DataFrame.from_dict(judges)\n",
    "    df.to_csv(f\"ffkkmo_judges_{str(season)[-2:]}{str(season+1)[-2:]}_dump.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
